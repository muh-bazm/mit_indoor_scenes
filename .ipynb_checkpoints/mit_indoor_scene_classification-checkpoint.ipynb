{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5cf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18224f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = 901 \n",
    "batch_size = 32\n",
    "\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db3c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc98639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir_2 = \"drive/MyDrive/projects/mit_indoor_scence_recognition/datasets/Images/*/*\"\n",
    "dataset_address = \"/home/ubuntu/projects/mit_indoor_scene/data/data_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9866d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 901 files belonging to 5 classes.\n",
      "Using 721 files for training.\n"
     ]
    }
   ],
   "source": [
    "training_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_address,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13099237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 901 files belonging to 5 classes.\n",
      "Using 180 files for validation.\n"
     ]
    }
   ],
   "source": [
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_address,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb9c034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = training_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd34562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_acc(y_true, y_pred):\n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    print(y_true)\n",
    "    print(\"----------------------------------------------\")\n",
    "    \n",
    "    Y_true = []\n",
    "    Y_pred = []\n",
    "\n",
    "    for i in range(y_true.shape[0]):\n",
    "        i_ = np.argmax(y_true[i,:])\n",
    "        j_ = np.argmax(y_pred[i,:])\n",
    "        Y_true.append(i_+1)\n",
    "        Y_pred.append(j_+1)\n",
    "    \n",
    "    return balanced_accuracy_score(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecfbd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "\n",
    "aug_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Input(shape=(256, 256, 3)),\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "  # tf.keras.layers.RandomHeight(0.2, interpolation='bilinear'),\n",
    "  tf.keras.layers.RandomTranslation(0.2, 0.2, fill_mode='reflect', interpolation='bilinear', fill_value=0.0),\n",
    "  # tf.keras.layers.RandomWidth(0.2, interpolation='bilinear'),\n",
    "  tf.keras.layers.RandomZoom(0.2, 0.2, fill_mode='reflect', interpolation='bilinear')  \n",
    "  # tf.keras.layers.Flatten()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32c56565",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input(shape=(256, 256, 3))\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "\n",
    "def get_model(input_layer_):\n",
    "\n",
    "  augmented_input = aug_model(input_layer_)\n",
    "  conv_1 = tf.keras.layers.Conv2D(512, 3, padding='same', activation='relu', input_shape=(256, 256, 3))(augmented_input)\n",
    "  conv_1 = tf.keras.layers.MaxPooling2D()(conv_1)\n",
    "\n",
    "  conv_2 = tf.keras.layers.Conv2D(256, 3, padding='same', activation='relu', input_shape=(256, 256, 3))(conv_1)\n",
    "  conv_2 = tf.keras.layers.MaxPooling2D()(conv_2)\n",
    "\n",
    "  conv_3 = tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu', input_shape=(256, 256, 3))(conv_2)\n",
    "  conv_3 = tf.keras.layers.MaxPooling2D()(conv_3)\n",
    "\n",
    "  conv_4 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu', input_shape=(256, 256, 3))(conv_3)\n",
    "  conv_4 = tf.keras.layers.MaxPooling2D()(conv_4)\n",
    "\n",
    "  conv_5 = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')(conv_4)\n",
    "  conv_5 = tf.keras.layers.MaxPooling2D()(conv_5)\n",
    "\n",
    "  conv_5 = tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu')(conv_5)\n",
    "  conv_5 = tf.keras.layers.MaxPooling2D()(conv_5)\n",
    "\n",
    "  flatten = tf.keras.layers.Flatten()(conv_5)\n",
    "  dense_1 = tf.keras.layers.Dense(256, activation='relu')(flatten)\n",
    "  dense_2 = tf.keras.layers.Dense(128, activation='relu')(dense_1)\n",
    "  dense_2 = tf.keras.layers.Dense(32, activation='relu')(dense_1)\n",
    "  dense_3 = tf.keras.layers.Dense(1, activation='softmax')(dense_2)\n",
    "\n",
    "  return dense_3\n",
    "\n",
    "\n",
    "last_layers = list()\n",
    "\n",
    "classes_num = 5\n",
    "\n",
    "for _ in range(classes_num):\n",
    "  last_layer = get_model(input_layer)\n",
    "  last_layers.append(last_layer)\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Concatenate()([ layer_ for layer_ in last_layers ])\n",
    "\n",
    "final_model = tf.keras.Model(input_layer, output_layer, name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8049cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[b_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3affe6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Tensor(\"IteratorGetNext:1\", shape=(None, 5), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/opt/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_2289/2755701899.py\", line 8, in b_acc  *\n        for i in range(y_true.shape[0]):\n\n    TypeError: 'NoneType' object cannot be interpreted as an integer\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filei05ueemv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_files4uveqw4.py:29\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__b_acc\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     27\u001b[0m i_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m i \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m ag__\u001b[38;5;241m.\u001b[39mfor_stmt(\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m, loop_body, get_state, set_state, (), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterate_names\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/opt/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_2289/2755701899.py\", line 8, in b_acc  *\n        for i in range(y_true.shape[0]):\n\n    TypeError: 'NoneType' object cannot be interpreted as an integer\n"
     ]
    }
   ],
   "source": [
    "history = final_model.fit(\n",
    "              train_ds,\n",
    "              validation_data=val_ds,\n",
    "              epochs=10,\n",
    "              verbose=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325a40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
